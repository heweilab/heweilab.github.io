<!doctype html>
<html
  dir="ltr"
  lang="zh"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <meta charset="utf-8" />
  <title>
    
      
        文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练 |
      何为

  </title>

  <meta name="generator" content="Hugo 0.155.2"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="何为" />
  <meta
    name="description"
    content="中国石油大学（华东）"
  />
  
  
        <link rel="stylesheet" href="/css/anatole.min.86b1f7f8e46fb7f9432e28081186d24eb6cd881e4d058e84aef93462f1e41337.css" integrity="sha256-hrH3&#43;ORvt/lDLigIEYbSTrbNiB5NBY6Ervk0YvHkEzc=" crossorigin="anonymous" />
      
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
    
    
    <link
      rel="stylesheet"
      href="/css/custom.min.9dbabbf9ac7efeff5e017d49cd0e09d4f995b3532048533b8f44341911a0c2f2.css"
      integrity="sha256-nbq7&#43;ax&#43;/v9eAX1JzQ4J1PmVs1MgSFM7j0Q0GRGgwvI="
      crossorigin="anonymous"
      media="screen"
    />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css"
    integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC&#43;3978NBRk="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css"
    integrity="sha256-5l3FtI&#43;185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css"
    integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR&#43;rP2S8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css"
    integrity="sha256-4QQlrXaLyY/x&#43;ycqCshCD50boi8GEsCP8QEMlQgP/n4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png" />

  <link rel="canonical" href="https://www.heweilab.com/zh/post/cellplm/" />
  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.8724ddf9268dee451060f191961647573c7f592fbccc6d858746236b3f915813.js"
      integrity="sha256-hyTd&#43;SaN7kUQYPGRlhZHVzx/WS&#43;8zG2Fh0Yjaz&#43;RWBM="
      crossorigin="anonymous"
    ></script>
  

  

  

  


  
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://www.heweilab.com/images/site-feature-image.png">
  <meta name="twitter:title" content="文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练">
  <meta name="twitter:description" content="密歇根州立大学
bioRxiv preprint doi: https://doi.org/10.1101/2023.10.03.560734; 该版本发布于2023年10月5日
Abstract 当前最先进的单细胞预训练模型深受大型语言模型成功的启发。这些模型通过将基因视为标记、细胞视为句子来训练transformer。然而，单细胞数据与自然语言数据之间存在三个根本差异常被忽视：(1)单细胞RNA测序数据以基因包形式呈现而非RNA序列；(2)细胞间关系比句子间关系更为复杂且重要；(3)单细胞数据量远少于文本数据且噪声较大。基于这些特性，我们提出了一种新型预训练模型CellPLM，该模型将细胞视为标记、组织视为句子。此外，我们在预训练中利用空间解析转录组数据来促进细胞间关系的学习，并引入高斯混合先验分布作为额外的归纳偏置以克服数据限制。CellPLM是首个编码细胞间关系的单细胞预训练transformer，在多种下游任务中持续超越现有预训练和非预训练模型，其推理速度比现有预训练模型快100倍。">



  
  <meta property="og:url" content="https://www.heweilab.com/zh/post/cellplm/">
  <meta property="og:site_name" content="我的博客">
  <meta property="og:title" content="文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练">
  <meta property="og:description" content="密歇根州立大学
bioRxiv preprint doi: https://doi.org/10.1101/2023.10.03.560734; 该版本发布于2023年10月5日
Abstract 当前最先进的单细胞预训练模型深受大型语言模型成功的启发。这些模型通过将基因视为标记、细胞视为句子来训练transformer。然而，单细胞数据与自然语言数据之间存在三个根本差异常被忽视：(1)单细胞RNA测序数据以基因包形式呈现而非RNA序列；(2)细胞间关系比句子间关系更为复杂且重要；(3)单细胞数据量远少于文本数据且噪声较大。基于这些特性，我们提出了一种新型预训练模型CellPLM，该模型将细胞视为标记、组织视为句子。此外，我们在预训练中利用空间解析转录组数据来促进细胞间关系的学习，并引入高斯混合先验分布作为额外的归纳偏置以克服数据限制。CellPLM是首个编码细胞间关系的单细胞预训练transformer，在多种下游任务中持续超越现有预训练和非预训练模型，其推理速度比现有预训练模型快100倍。">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-09-04T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-04T00:00:00+00:00">
    <meta property="og:image" content="https://www.heweilab.com/images/site-feature-image.png">



  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练",
        "headline": "文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练",
        "alternativeHeadline": "",
        "description": "
      
        \u003cp\u003e\u003cimg src=\u0022\/picture\/CellPLM\/1756887060262-415637cd-0973-4b41-8c58-3576fbf3b05f.png\u0022 alt=\u0022\u0022\u003e\u003c\/p\u003e\n\u003cp\u003e密歇根州立大学\u003c\/p\u003e\n\u003cp\u003ebioRxiv preprint doi: \u003ca href=\u0022https:\/\/doi.org\/10.1101\/2023.10.03.560734;\u0022\u003ehttps:\/\/doi.org\/10.1101\/2023.10.03.560734;\u003c\/a\u003e 该版本发布于2023年10月5日\u003c\/p\u003e\n\u003ch1 id=\u0022abstract\u0022\u003e\u003cfont style=\u0022color:rgb(0, 0, 0);\u0022\u003eAbstract\u003c\/font\u003e\u003c\/h1\u003e\n\u003cp\u003e当前最先进的单细胞预训练模型深受大型语言模型成功的启发。这些模型通过将基因视为标记、细胞视为句子来训练transformer。然而，单细胞数据与自然语言数据之间存在三个根本差异常被忽视：(1)单细胞RNA测序数据以基因包形式呈现而非RNA序列；(2)细胞间关系比句子间关系更为复杂且重要；(3)单细胞数据量远少于文本数据且噪声较大。基于这些特性，我们提出了一种新型预训练模型CellPLM，该模型将细胞视为标记、组织视为句子。此外，我们在预训练中利用空间解析转录组数据来促进细胞间关系的学习，并引入高斯混合先验分布作为额外的归纳偏置以克服数据限制。CellPLM是首个编码细胞间关系的单细胞预训练transformer，在多种下游任务中持续超越现有预训练和非预训练模型，其推理速度比现有预训练模型快100倍。\u003c\/p\u003e


      


    ",
        "license": "",
        
        "inLanguage": "zh",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/www.heweilab.com\/zh\/post\/cellplm\/"
        },
        "author" : {
            "@type": "Person",
            "name": "何为"
        },
        "creator" : {
            "@type": "Person",
            "name": "何为"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "何为"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "何为"
        },
        "dateCreated": "2025-09-04T00:00:00.00Z",
        "datePublished": "2025-09-04T00:00:00.00Z",
        "dateModified": "2025-09-04T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "何为",
            "url": "https://www.heweilab.com/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/www.heweilab.com\/favicons\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "https://www.heweilab.com/images/site-feature-image.png"


      
      ]

    ,
        "url" : "https:\/\/www.heweilab.com\/zh\/post\/cellplm\/",
        "wordCount" : "95",
        "genre" : [ ],
        "keywords" : [ ]
    }
  </script>




  
  
  
</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/zh">何为</a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>中国石油大学（华东）</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a
            href="https://scholar.google.com/citations?view_op=list_works&amp;hl=zh-CN&amp;authuser=1&amp;user=X_5m9FoAAAAJ"
            target="_blank"
            rel="noopener me"
            aria-label="Google Scholar"
            title="Google Scholar"
          >
            <i class="fa-solid fa-graduation-cap fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://github.com/heweilab"
            target="_blank"
            rel="noopener me"
            aria-label="GitHub"
            title="GitHub"
          >
            <i class="fa-brands fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="mailto:heweilab@gmail.com"
            target="_blank"
            rel="noopener me"
            aria-label="E-mail"
            title="E-mail"
          >
            <i class="fa-solid fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href=""
            target="_blank"
            rel="noopener me"
            aria-label="WeChat"
            title="WeChat"
          >
            <i class="fa-brands fa-weixin fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://qm.qq.com/q/xwBIUieW8U"
            target="_blank"
            rel="noopener me"
            aria-label="QQ"
            title="QQ"
          >
            <i class="fa-brands fa-qq fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
    </ul>

  </div>
  <div class="sidebar__footer">
    





  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        何为
        2026
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.9531b4a2217a70c5a1fce89c1f81c9ebbdd586708fcd4130b417320c7230c8d6.js"
    integrity="sha256-lTG0oiF6cMWh/OicH4HJ673VhnCPzUEwtBcyDHIwyNY="
    crossorigin="anonymous"
  ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/zh/"
              
              title=""
              >首页</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/zh/post/"
              
              title=""
              >文章</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/zh/cv/"
              
              title=""
              >简历</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      
        <h1>文献阅读 | CellPLM：超越单细胞的细胞语言模型预训练</h1>
      
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                
                  4/9/2025
                

              
            </span>
          </li>
        </ul>
      <p><img src="/picture/CellPLM/1756887060262-415637cd-0973-4b41-8c58-3576fbf3b05f.png" alt=""></p>
<p>密歇根州立大学</p>
<p>bioRxiv preprint doi: <a href="https://doi.org/10.1101/2023.10.03.560734;">https://doi.org/10.1101/2023.10.03.560734;</a> 该版本发布于2023年10月5日</p>
<h1 id="abstract"><font style="color:rgb(0, 0, 0);">Abstract</font></h1>
<p>当前最先进的单细胞预训练模型深受大型语言模型成功的启发。这些模型通过将基因视为标记、细胞视为句子来训练transformer。然而，单细胞数据与自然语言数据之间存在三个根本差异常被忽视：(1)单细胞RNA测序数据以基因包形式呈现而非RNA序列；(2)细胞间关系比句子间关系更为复杂且重要；(3)单细胞数据量远少于文本数据且噪声较大。基于这些特性，我们提出了一种新型预训练模型CellPLM，该模型将细胞视为标记、组织视为句子。此外，我们在预训练中利用空间解析转录组数据来促进细胞间关系的学习，并引入高斯混合先验分布作为额外的归纳偏置以克服数据限制。CellPLM是首个编码细胞间关系的单细胞预训练transformer，在多种下游任务中持续超越现有预训练和非预训练模型，其推理速度比现有预训练模型快100倍。</p>
<hr>
<p><strong><font style="color:rgb(15, 17, 21);">1. 当前主流做法与局限性（第一、二句）</font></strong></p>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">当前主流做法</font></strong><font style="color:rgb(15, 17, 21);">：目前最先进的单细胞预训练模型（如Geneformer）深受大型语言模型（如GPT）成功的启发。它们模仿自然语言处理（NLP）的方法：</font>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">基因 (Gene) = 词汇 (Token)</font></strong><font style="color:rgb(15, 17, 21);">：把一个一个的基因看作是语言中的一个一个的单词。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">细胞 (Cell) = 句子 (Sentence)</font></strong><font style="color:rgb(15, 17, 21);">：把一个细胞（内部包含了成千上万个基因的表达量）看作是这些“单词”组成的一个“句子”。</font></li>
<li><font style="color:rgb(15, 17, 21);">这样，就可以用训练Transformer（如BERT）的方式来训练模型，让它学习“基因语言”的语法和上下文。</font></li>
</ul>
</li>
</ul>
<p><strong><font style="color:rgb(15, 17, 21);">2. 作者指出的三个根本性缺陷（第三至五句）</font></strong></p>
<p><font style="color:rgb(15, 17, 21);">作者认为，生搬硬套NLP模型忽略了单细胞数据与自然语言数据的</font><strong><font style="color:rgb(15, 17, 21);">三个本质区别</font></strong><font style="color:rgb(15, 17, 21);">，这是现有模型的致命弱点：</font></p>
<ol>
<li><strong><font style="color:rgb(15, 17, 21);">“词袋”而非序列 (Bag-of-genes vs. Sequence)</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">自然语言</font></strong><font style="color:rgb(15, 17, 21);">：单词的顺序至关重要。“猫追狗”和“狗追猫”意思完全不同。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">单细胞数据</font></strong><font style="color:rgb(15, 17, 21);">：传统scRNA-seq技术在测量时</font><strong><font style="color:rgb(15, 17, 21);">丢失了基因分子的顺序信息</font></strong><font style="color:rgb(15, 17, 21);">。一个细胞的数据更像一个“基因袋”，我们只知道里面有哪些基因以及它们的表达量高低，但不知道它们谁先谁后。因此，用Transformer来学习“基因序列”的上下文，其基础就受到了挑战。</font></li>
</ul>
</li>
<li><strong><font style="color:rgb(15, 17, 21);">细胞关系远比句子关系复杂 (Intricate Cell-Cell Relations)</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">自然语言</font></strong><font style="color:rgb(15, 17, 21);">：句子与句子之间通常是独立的，一篇文章中的上下句有关联，但和另一本书里的句子基本没关系。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">生物学</font></strong><font style="color:rgb(15, 17, 21);">：细胞与细胞之间存在着极其精密和重要的空间邻接、通信（如配体-受体互作）、发育 lineage 等关系。理解一个细胞，必须把它放在其所在的“细胞社会”（组织/微环境）中来看。现有模型忽略了这点。</font></li>
</ul>
</li>
<li><strong><font style="color:rgb(15, 17, 21);">数据量小且噪声大 (Data Quantity and Noise)</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">文本数据</font></strong><font style="color:rgb(15, 17, 21);">：互联网上有海量高质量文本（万亿级词汇）可用于训练LLM。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">单细胞数据</font></strong><font style="color:rgb(15, 17, 21);">：相比之下，可用的单细胞数据量非常少（百万级细胞），而且测量技术本身会引入大量噪声和技术偏差。在有限且嘈杂的数据上训练庞大的Transformer模型，很容易过拟合或效果不佳。</font></li>
</ul>
</li>
</ol>
<p><strong><font style="color:rgb(15, 17, 21);">3. 作者的解决方案：CellPLM（第六至八句）</font></strong></p>
<p><font style="color:rgb(15, 17, 21);">针对以上三个问题，作者提出了全新的模型</font><strong><font style="color:rgb(15, 17, 21);">CellPLM</font></strong><font style="color:rgb(15, 17, 21);">：</font></p>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">颠覆性的建模思路</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ul>
<li><font style="color:rgb(15, 17, 21);">他们</font><strong><font style="color:rgb(15, 17, 21);">反转了“Token”和“Sentence”的定义</font></strong><font style="color:rgb(15, 17, 21);">。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">细胞 (Cell) = 词汇 (Token)</font></strong><font style="color:rgb(15, 17, 21);">：现在，每一个细胞被看作是一个基本的“单词”。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">组织 (Tissue) = 句子 (Sentence)</font></strong><font style="color:rgb(15, 17, 21);">：由许多细胞组成的组织（或样本）被看作是由这些“细胞单词”组成的“句子”。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">这样做的好处</font></strong><font style="color:rgb(15, 17, 21);">：模型天然的学习单元就变成了</font><strong><font style="color:rgb(15, 17, 21);">细胞</font></strong><font style="color:rgb(15, 17, 21);">，其直接目标就是学习细胞之间的关系和整个组织的上下文，完美对应了上述第二个缺陷。</font></li>
</ul>
</li>
<li><strong><font style="color:rgb(15, 17, 21);">两大技术创新</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ol>
<li><strong><font style="color:rgb(15, 17, 21);">利用空间转录组数据 (Spatially-resolved data)</font></strong><font style="color:rgb(15, 17, 21);">：在预训练阶段就加入能揭示细胞</font><strong><font style="color:rgb(15, 17, 21);">真实空间位置</font></strong><font style="color:rgb(15, 17, 21);">的数据。这为模型提供了细胞间物理邻接关系的“地面真相”，极大地促进了模型对复杂细胞关系的学习。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">引入高斯混合先验 (Gaussian mixture prior)</font></strong><font style="color:rgb(15, 17, 21);">：这是一种数学上的“归纳偏置”（Inductive Bias），可以理解为根据生物学先验知识给模型一个“提示”或“约束”。它假设细胞类型服从一个由几个高斯分布混合而成的概率模型（因为生物细胞确实可以分成不同的类别或状态）。这就像在教模型之前先告诉它：“数据大概长这样，你照着这个方向去学”，从而帮助模型在</font><strong><font style="color:rgb(15, 17, 21);">数据量小且噪声大</font></strong><font style="color:rgb(15, 17, 21);">的情况下更高效、更鲁棒地学习到本质特征，而不是被噪声带偏。</font></li>
</ol>
</li>
</ul>
<p><strong><font style="color:rgb(15, 17, 21);">4. 论文的贡献与成果（最后两句）</font></strong></p>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">核心贡献</font></strong><font style="color:rgb(15, 17, 21);">：CellPLM是</font><strong><font style="color:rgb(15, 17, 21);">第一个</font></strong><font style="color:rgb(15, 17, 21);">能够编码细胞-细胞关系的单细胞预训练Transformer模型。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">卓越性能</font></strong><font style="color:rgb(15, 17, 21);">：</font>
<ul>
<li><strong><font style="color:rgb(15, 17, 21);">效果更好</font></strong><font style="color:rgb(15, 17, 21);">：在多种下游任务（如细胞类型注释、疾病状态预测等）中，其性能</font><strong><font style="color:rgb(15, 17, 21);">持续且稳定地超越</font></strong><font style="color:rgb(15, 17, 21);">了现有的（有预训练的）和传统的（没有预训练的）模型。</font></li>
<li><strong><font style="color:rgb(15, 17, 21);">速度极快</font></strong><font style="color:rgb(15, 17, 21);">：其</font><strong><font style="color:rgb(15, 17, 21);">推理速度（Inference speed）比现有预训练模型快100倍</font></strong><font style="color:rgb(15, 17, 21);">。这是一个巨大的工程优势，意味着它能够快速处理大规模数据，实用性极强。</font></li>
</ul>
</li>
</ul>
<hr>
<h1 id="introduction"><font style="color:#000000;">Introduction</font></h1>
<p>单细胞RNA测序（scRNA-seq）等新一代测序技术产生了海量数据，这引发了开发大规模预训练模型进行单细胞分析的热潮（杨等人，2022；龚等人，2023；沈等人，2023；崔等人，2023；西奥多里斯等人，2023)。这些模型旨在从未标注的单细胞RNA测序数据中捕捉潜在结构和规律，并可通过特定下游数据集进行微调，从而实现精准预测并深入解析细胞机制。值得注意的是，这类预训练模型借鉴了BERT和GPT等大型语言模型的成功经验（肯顿与图塔诺瓦，2019；布贝克等人，2023)，将基因视为单词（标记），将细胞视为句子来训练Transformer模型（瓦斯瓦尼等人，2017)。然而我们认为，由于单细胞数据与自然语言数据存在根本性差异，现有文献对此的忽视可能导致这些方法存在局限性：</p>
<hr>
<p><em><u>现有工作：数据增加引发大规模预训练模型的热潮，模型借鉴BERT和GPT等大语言模型，将基因视为单词，将细胞视为句子训练Transformer，但是这样做存在局限：</u></em></p>
<hr>
<p>首先，与传统句子不同，现有预训练模型使用的单细胞RNA测序（scRNA-seq）数据不具备连续性。在训练阶段前，RNA序列已被识别为功能单元（即基因）。这些数据不再以原始序列形式呈现，而是转化为细胞-基因计数矩阵，用于量化每个细胞内各基因的丰度。这种处理方式类似于自然语言中的词袋模型——基因集合是固定不变的，且各基因之间不存在顺序关联。</p>
<p>其次，细胞间的相互作用关系比句子间的关系更为复杂且重要，因为细胞间的通讯在决定细胞状态和发育过程中起着关键作用（阿明戈尔等人，2021)。此外，在组织内部，来自相同或相似细胞谱系的大量细胞具有相似的基因表达特征，这为细胞状态的去噪和识别提供了宝贵补充信息（坎诺特等人，2016；莫尔霍等人，2022；斯特里特等人，2018)。因此，近年来许多研究方法（王等人，2021；邵等人，2022；徐等人，2023；文等人，2023)都构建了细胞间图谱来推进单细胞数据的表征学习。这些证据表明，现有预训练模型忽视的细胞间关系具有重要价值。</p>
<p>第三，单细胞数据集的数量和质量明显逊色于自然语言数据。以英文数据为例，从公共爬虫语料库中提取的高质量过滤数据集（Wenzek等人，2020）包含320亿条句子，而规模最大的单细胞数据集——人类细胞图谱（Regev等人，2017）仅包含不到5000万个细胞。更棘手的是，单细胞数据常受技术伪影和数据丢失事件影响（Svensson等人，2017；邱，2020），同时不同测序平台和实验间的批次效应也显著存在（Tran等人，2020；Argelaguet等人，2021）。</p>
<hr>
<p><em><u>1、RNA测序数据以细胞-基因计数矩阵形式存储，不具备原始序列的连续性，只是类似词袋，而非句子。</u></em></p>
<p><em><u>2、细胞间的互作比句子间互作更为复杂且重要</u></em></p>
<p><em><u>3、单细胞数据集的数量和质量不如自然语言数据，且单细胞数据存在技术伪影</u></em></p>
<hr>
<p>上述差异带来了独特的挑战，需要针对单细胞数据量身定制新的预训练策略。为弥合这一差距，我们提出了一种新型单细胞预训练语言模型（CellPLM），从以下角度应对这些挑战：首先，如图1所示，CellPLM通过构建细胞间关系的语言模型来解决细胞间关联问题。由于基因表达属于词袋特征，其嵌入向量通过聚合基因嵌入进行初始化。其次，CellPLM利用新型数据——<font style="background-color:#FBDE28;">空间解析转录组数据（SRT）</font>——为揭示细胞间相互作用提供额外参考。相较于单细胞RNA测序数据，SRT数据能为细胞提供更丰富的定位信息。这两种数据类型通过transformer共同建模。第三，CellPLM引入归纳偏置机制，通过在潜在空间中采用高斯混合模型作为先验分布，克服了数据量和质量的限制。该设计可生成更平滑、更优质的细胞潜在表征（Grønbech等，2020；Xu等，2023；Jiang等，2023)。据我们所知，CellPLM是首个同时编码细胞间关系、利用空间解析转录组数据并采用合理先验分布的预训练transformer框架。实验数据表明，CellPLM在五个下游任务中持续超越预训练和非预训练方法，其推理速度比现有预训练模型快100倍。为确保可复现性，我们的模型源代码和检查点将发布在Github2平台。</p>
<p><img src="/picture/CellPLM/1756901144759-1f70a24b-1615-41a8-96e0-21cec4cd55c3.png" alt=""></p>
<p><em>图1：现有单细胞预训练模型与CellPLM在语言模型差异的示意图。现有预训练模型仅考虑同一细胞内基因表达的条件概率，而CellPLM则将基因表达分布也与其他细胞的条件相结合。详见第3节</em></p>
<hr>
<p><em><u>解决方案CellPLM：</u></em></p>
<p><em><u>1、构建细胞间关系语言模型，将基因表达量当做词袋特征进行聚合基因嵌入初始化</u></em></p>
<p><em><u>2、利用空间解析转录组数据（SRT），引入细胞定位信息</u></em></p>
<p><em><u>3、引入归纳偏置机制，解决数据的数量和质量限制</u></em></p>
<p><em><u><font style="background-color:#FBDE28;">空间解析转录组数据（</font></u><strong><u><font style="color:rgb(0,0,0);background-color:#FBDE28;">spatially-resolved transcriptomic，</font></u></strong><u><font style="background-color:#FBDE28;">SRT）：</font></u>__<u><font style="color:rgb(15, 17, 21);">简单来说，空间转录组（ST）是一个更广泛的总称，而空间解析转录组（SRT）是其一个更高级、分辨率更高的子集。所有的SRT都是ST，但并非所有的ST都能被称为高解析度的SRT。SRT的分辨率更高</font></u></em></p>
<hr>
</div>
    <div class="post__footer">
      

      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        何为
        2026
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.9531b4a2217a70c5a1fce89c1f81c9ebbdd586708fcd4130b417320c7230c8d6.js"
    integrity="sha256-lTG0oiF6cMWh/OicH4HJ673VhnCPzUEwtBcyDHIwyNY="
    crossorigin="anonymous"
  ></script></body>
</html>
